# # -*- coding: utf-8 -*-
# """GloVec.ipynb

# Automatically generated by Colaboratory.

# Original file is located at
#     https://colab.research.google.com/drive/1Dr7DdrEFWy8EP4sNW_hOLLm5Kg9Z8VKa
# """

# !pip install pyvi
# !pip install glove-python3
# !pip install np_utils
# !pip install underthesea
path = '/content/drive/MyDrive/WE-GloVe/'

# from google.colab import drive
# drive.mount('/content/drive')

from typing import final
from glove import Corpus, Glove
import pandas as pd
import numpy as np
import re
import tensorflow as tf
from tensorflow.keras.models import Sequential
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.layers import Embedding,Bidirectional, LSTM, Flatten, Dense,Input,Average,Reshape,Dropout,Concatenate
from tensorflow.keras.layers import Conv2D, MaxPool2D
from tensorflow.keras import utils
from underthesea import word_tokenize, sent_tokenize

# device_name = tf.test.gpu_device_name()
# if device_name != '/device:GPU:0':
#   raise SystemError('GPU device not found')
# print('Found GPU at: {}'.format(device_name))

# """Train **data**"""

file_train_content = pd.read_csv(path + 'content_train.csv')
# file_train_content = file_train_content.sort_values(by='Rating', ascending=True)
# file_train_title = pd.read_csv(path + 'title_train.csv')
# file_train_title = file_train_title.sort_values(by='Label', ascending=True)

x_train_content = file_train_content['Content'].apply(str)
y_train_content = file_train_content['Rating']
train_labels = utils.to_categorical(y_train_content-1, num_classes=5)

# x_train_title = file_train_title['Text'].apply(str)
# y_train_title = file_train_title['Label']

max_len_title = 30
max_len_text = 300
EMBEDDING_DIM = 300

# file_train_title

# file_train_content

# """Test **data**"""

file_test_content = pd.read_csv(path + 'content_test.csv')
# file_test_content = file_test_content.sort_values(by='Rating', ascending=True)
# file_test_title = pd.read_csv(path + 'title_test.csv')
# file_test_title = file_test_title.sort_values(by='Label', ascending=True)

x_test_content = file_test_content['Content'].apply(str)
y_test_content = file_test_content['Rating']
test_labels = utils.to_categorical(y_test_content-1, num_classes=5)

# x_test_title = file_test_title['Text'].apply(str)
# y_test_title = file_test_title['Label']

# file_test_title

# file_test_content

# """**Pre-process Data For Training Glove Model**"""

def preprocess_data(sentence):
  sentence = word_tokenize(sentence, format='text')
  sentence = re.sub(r'[^\w\s]', '', sentence.lower())
  return sentence.lower()

x_train_content = [preprocess_data(sentence) for sentence in x_train_content if sentence != 'nan' and sentence != '']

tokenizer = Tokenizer()

def get_sequences(data):
  tokenizer.fit_on_texts(data)
  return tokenizer.texts_to_sequences(data)

train_text = get_sequences(x_train_content)
train_text = pad_sequences(train_text, padding = 'post', maxlen = max_len_text)
test_text = get_sequences(x_test_content)
test_text = pad_sequences(test_text, padding = 'post', maxlen = max_len_text)

# x_train

# vocab = []
# for i in x_train:
#   s = i.split()
#   for j in s:
#     if j not in vocab:
#       vocab.append(j)

# print(len(vocab))

# """TRAINING GLOVE MODEL"""

corpus = Corpus()
corpus.fit([sentence.split() for sentence in x_train_content], window = 15)

glove = Glove(no_components=EMBEDDING_DIM, learning_rate=0.05)
glove.fit(corpus.matrix, epochs=30, no_threads=4, verbose=True)
glove.add_dictionary(corpus.dictionary)

word_vectors = glove.word_vectors
word_dictionary = glove.dictionary

glove.save(path + 'gloveModel.model')

# glove.most_similar('t√¥i', 10)

# """**Prepare For Building Model**"""



embeddings_dictionary = dict()
gloveModel = Glove.load(path + 'gloveModel.model')

embedding_keys = list(gloveModel.dictionary.keys())
vocab_size = len(gloveModel.dictionary)

for word in embedding_keys:
    embeddings_dictionary[word] = gloveModel.word_vectors[gloveModel.dictionary[word]]

# vocab_size

embedding_matrix = np.zeros((vocab_size, EMBEDDING_DIM))
for item in gloveModel.dictionary:
    embedding_vector = embeddings_dictionary.get(item)
    if embedding_vector is not None:
        embedding_matrix[gloveModel.dictionary.get(item)] = embedding_vector

# """## **LSTM + CNN**"""

filter_sizes = [3,4,5]
num_filters = 300
drop = 0.5
def build_classifier_model_lstm_cnn():
  inputs = Input(shape=(max_len_text,), dtype='int32')
  embedding = Embedding(input_dim=vocab_size, output_dim=EMBEDDING_DIM, weights=[embedding_matrix],
                        input_length=max_len_text, trainable=False)(inputs)
  lstm = LSTM(128,activation = 'tanh', return_sequences = True)(embedding)
  reshape = Reshape((max_len_text,128,1))(lstm)

  conv_0 = Conv2D(num_filters, kernel_size=(filter_sizes[0], 128), padding='valid', kernel_initializer='normal', activation='relu')(reshape)
  conv_1 = Conv2D(num_filters, kernel_size=(filter_sizes[1], 128), padding='valid', kernel_initializer='normal', activation='relu')(reshape)
  conv_2 = Conv2D(num_filters, kernel_size=(filter_sizes[2], 128), padding='valid', kernel_initializer='normal', activation='relu')(reshape)

  maxpool_0 = MaxPool2D(pool_size=(max_len_text - filter_sizes[0] + 1, 1), strides=(1,1), padding='valid')(conv_0)
  maxpool_1 = MaxPool2D(pool_size=(max_len_text - filter_sizes[1] + 1, 1), strides=(1,1), padding='valid')(conv_1)
  maxpool_2 = MaxPool2D(pool_size=(max_len_text - filter_sizes[2] + 1, 1), strides=(1,1), padding='valid')(conv_2)

  concatenated_tensor = Concatenate(axis=1)([maxpool_0, maxpool_1, maxpool_2])
  flatten = Flatten()(concatenated_tensor)
  dropout = Dropout(drop)(flatten)
  preds = Dense(5, activation='softmax', name = 'classifier')(dropout)

  # this creates a model that includes inputs and outputs
  model = tf.keras.Model(inputs=inputs, outputs=preds)

  model.compile(loss='categorical_crossentropy',
                optimizer='adam',
                metrics=['acc'])
  return model
  # model.summary()

model = build_classifier_model_lstm_cnn()
model.summary()

final_model = None
for i in range(1,6):
  model = build_classifier_model_lstm_cnn()
  history = model.fit(train_text, train_labels, batch_size = 128, epochs = 5, validation_split = 0.10, verbose = 1)
  score = model.evaluate(test_text, test_labels, batch_size=128, verbose = 1)
  final_model = model