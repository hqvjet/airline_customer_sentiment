# -*- coding: utf-8 -*-
"""Hotel Customer's Sentiment Analyse

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Dr7DdrEFWy8EP4sNW_hOLLm5Kg9Z8VKa

## **GETTING STARTED**
"""

!pip install glove-python3
!pip install underthesea
path = '/content/drive/MyDrive/WE-GloVe/'

from google.colab import drive
drive.mount('/content/drive')

from glove import Corpus, Glove
import pandas as pd
import numpy as np
import re
from underthesea import word_tokenize, sent_tokenize
import math
from tensorflow.keras import utils
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.preprocessing.text import Tokenizer
import seaborn
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Flatten, Dense
from tensorflow.keras import utils
from keras.layers import Input, Embedding, LSTM, Dropout, Dense, concatenate
from keras.models import Model

# device_name = tf.test.gpu_device_name()
# if device_name != '/device:GPU:0':
#   raise SystemError('GPU device not found')
# print('Found GPU at: {}'.format(device_name))

"""## **GET TRAIN DATA**"""

# Dataset Prepare
def getData(file_name):
  file = pd.read_csv(path + file_name)

  title = pd.Series([re.sub(r'\s+', ' ', sent) for sent in file['title'].apply(str)])
  text = pd.Series([re.sub(r'\s+', ' ', sent) for sent in file['text'].apply(str)])

  return title, text, utils.to_categorical(file['rating'] - 1, num_classes=5)

x_train_title, x_train_text, y_train = getData('train.csv')
x_test_title, x_test_text, y_test = getData('test.csv')

# Cho thấy bộ dữ liệu phân bố không đồng đều
# seaborn.countplot(x = 'rating', data = train_file)

"""## **DATA PREPARING STAGE**

Tokenize data
"""

def tokenize_data(title, text):
  arr_title = [word_tokenize(sentence, format='text') for sentence in title]
  arr_text = [word_tokenize(sentence, format='text') for sentence in text]

  return arr_title, arr_text

x_train_title, x_train_text = tokenize_data(x_train_title, x_train_text)

"""Convert to sequences"""

tokenizer = Tokenizer()

tokenizer.fit_on_texts([x_train_title, x_train_text])

x_train_title_sequence = tokenizer.texts_to_sequences(x_train_title)
x_train_text_sequence = tokenizer.texts_to_sequences(x_train_text)
x_test_title_sequence = tokenizer.texts_to_sequences(x_test_title)
x_test_text_sequence = tokenizer.texts_to_sequences(x_test_text)

"""Padding sequences to the same dimensions"""

vocab_size = len(tokenizer.word_index) + 1
MAX_LEN = 512

x_train_title_pad = pad_sequences(x_train_title_sequence, padding = 'post', maxlen = MAX_LEN)
x_train_text_pad = pad_sequences(x_train_text_sequence, padding = 'post', maxlen = MAX_LEN)
x_test_title_pad = pad_sequences(x_test_title_sequence, padding = 'post', maxlen = MAX_LEN)
x_test_text_pad = pad_sequences(x_test_text_sequence, padding = 'post', maxlen = MAX_LEN)

"""## **GLOVE EMBEDDING IMPLEMENTATION AND USAGE**"""

glove = Glove.load(path + 'gloveModel.model')
emb_dict = dict()

for word in list(glove.dictionary.keys()):
  emb_dict[word] = glove.word_vectors[glove.dictionary[word]]

emb_matrix = np.zeros((vocab_size, MAX_LEN))
for word, index in tokenizer.word_index.items():
  emb_vector = emb_dict.get(word)
  if emb_vector is not None:
    emb_matrix[index] = emb_vector

EPOCH = 100
BATCH_SIZE = 4

# """## **LSTM**

# Define layer
# """

# embedding_dim = 128
# hidden_size = 256

# # Đầu vào cho title
# title_input = Input(shape=(x_train_title_pad.shape[1],))
# title_embedding = Embedding(vocab_size, embedding_dim, input_length=x_train_title_pad.shape[1])(title_input)
# title_lstm = LSTM(hidden_size, return_sequences=True)(title_embedding)
# title_lstm_dropout = Dropout(0.2)(title_lstm)
# title_lstm_final = LSTM(hidden_size)(title_lstm_dropout)

# # Đầu vào cho text
# text_input = Input(shape=(x_train_text_pad.shape[1],))
# text_embedding = Embedding(vocab_size, embedding_dim, input_length=x_train_text_pad.shape[1])(text_input)
# text_lstm = LSTM(hidden_size, return_sequences=True)(text_embedding)
# text_lstm_dropout = Dropout(0.2)(text_lstm)
# text_lstm_final = LSTM(hidden_size)(text_lstm_dropout)

# # Kết hợp hai đầu vào
# combined = concatenate([title_lstm_final, text_lstm_final])

# # Các bước còn lại của mô hình
# dense1 = Dense(128, activation='relu')(combined)
# output = Dense(y_train.shape[1], activation='softmax')(dense1)

# # Xây dựng mô hình
# model_LSTM = Model(inputs=[title_input, text_input], outputs=output)

# model_LSTM.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
# model_LSTM.summary()

# from keras.utils import plot_model

# plot_model(model_LSTM, to_file='model.png', show_shapes=True, show_layer_names=True)

# model_LSTM_history = model_LSTM.fit(
#     [np.array(x_train_title_pad), np.array(x_train_text_pad)],
#     y_train,
#     epochs=EPOCH,
#     batch_size=4,
#     verbose=1,
#     validation_data=([np.array(x_test_title_pad), np.array(x_test_text_pad)], y_test)
# )

# """# **CNN**"""

# from keras.layers import Input, Embedding, Conv1D, MaxPooling1D, Flatten, Dense, concatenate
# from keras.models import Model

# embedding_dim = 128
# num_filters = 128
# filter_sizes = [3, 4, 5]

# # Input for title
# title_input = Input(shape=(x_train_title_pad.shape[1],))
# title_embedding = Embedding(vocab_size, embedding_dim, input_length=x_train_title_pad.shape[1])(title_input)
# title_conv_blocks = []
# for filter_size in filter_sizes:
#     title_conv = Conv1D(filters=num_filters, kernel_size=filter_size, activation='relu')(title_embedding)
#     title_pool = MaxPooling1D(pool_size=x_train_title_pad.shape[1] - filter_size + 1)(title_conv)
#     title_conv_blocks.append(title_pool)
# title_concat = concatenate(title_conv_blocks, axis=-1)
# title_flat = Flatten()(title_concat)

# # Input for text
# text_input = Input(shape=(x_train_text_pad.shape[1],))
# text_embedding = Embedding(vocab_size, embedding_dim, input_length=x_train_text_pad.shape[1])(text_input)
# text_conv_blocks = []
# for filter_size in filter_sizes:
#     text_conv = Conv1D(filters=num_filters, kernel_size=filter_size, activation='relu')(text_embedding)
#     text_pool = MaxPooling1D(pool_size=x_train_text_pad.shape[1] - filter_size + 1)(text_conv)
#     text_conv_blocks.append(text_pool)
# text_concat = concatenate(text_conv_blocks, axis=-1)
# text_flat = Flatten()(text_concat)

# # Combine the two inputs
# combined = concatenate([title_flat, text_flat])

# # Additional layers of the model
# dense1 = Dense(128, activation='relu')(combined)
# output = Dense(y_train.shape[1], activation='softmax')(dense1)

# # Build the model
# model_CNN = Model(inputs=[title_input, text_input], outputs=output)

# model_CNN.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
# model_CNN.summary()

# from keras.utils import plot_model

# plot_model(model_CNN, to_file='modelCNN.png', show_shapes=True, show_layer_names=True)

# def train_model(model, X_train, y_train, X_val, y_val, epochs=10, batch_size=32):
#     history = model.fit(
#         [X_train['title'], X_train['text']],
#         y_train,
#         validation_data=([X_val['title'], X_val['text']], y_val),
#         epochs=epochs,
#         batch_size=batch_size
#     )
#     return history

# history = model_CNN.fit(
#     [np.array(x_train_title_pad), np.array(x_train_text_pad)],
#     y_train,
#     epochs=EPOCH,
#     batch_size=4,
#     verbose=1,
#     validation_data=([np.array(x_test_title_pad), np.array(x_test_text_pad)], y_test)
# )

# """# **BiLSTM**"""

# from keras.layers import Input, Bidirectional, LSTM, Dense, GlobalMaxPooling1D
# from keras.models import Model

# def build_bilstm_model():
#     # Define input layers for the title and text inputs
#     title_input = Input(shape=(x_train_title_pad.shape[1],))
#     text_input = Input(shape=(x_train_text_pad.shape[1],))

#     # Embedding layer for title
#     title_embedding = Embedding(input_dim=vocab_size, output_dim=embedding_dim, trainable=True)(title_input)
#     # Embedding layer for text
#     text_embedding = Embedding(input_dim=vocab_size, output_dim=embedding_dim, trainable=True)(text_input)

#     # Bidirectional LSTM layer for title
#     title_bilstm = Bidirectional(LSTM(64, return_sequences=True))(title_embedding)
#     # Bidirectional LSTM layer for text
#     text_bilstm = Bidirectional(LSTM(64, return_sequences=True))(text_embedding)

#     # Global Max Pooling layer for title
#     title_pooling = GlobalMaxPooling1D()(title_bilstm)
#     # Global Max Pooling layer for text
#     text_pooling = GlobalMaxPooling1D()(text_bilstm)

#     # Concatenate title and text pooling layers
#     concatenated_pooling = concatenate([title_pooling, text_pooling])

#     # Dense layer for final prediction
#     output_layer = Dense(5, activation='softmax')(concatenated_pooling)

#     # Create model
#     model = Model(inputs=[title_input, text_input], outputs=output_layer)

#     return model

# # Build the BiLSTM model
# model_BiLSTM = build_bilstm_model()
# model_BiLSTM.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
# model_BiLSTM.summary()

# from keras.utils import plot_model

# plot_model(model_BiLSTM, to_file='modelBiLSTM.png', show_shapes=True, show_layer_names=True)

# history = model_BiLSTM.fit(
#     [np.array(x_train_title_pad), np.array(x_train_text_pad)],
#     y_train,
#     epochs=EPOCH,
#     batch_size=4,
#     verbose=1,
#     validation_data=([np.array(x_test_title_pad), np.array(x_test_text_pad)], y_test)
# )

# """# **BiLSTM + CNN**"""

from keras.layers import Input, concatenate, Dense, Concatenate, Bidirectional, LSTM, Conv1D, GlobalMaxPooling1D
from keras.models import Model
from keras import backend as K

def build_ensemble_model(model_BiLSTM, model_CNN):
    # Define input layers for the title and text inputs
    title_input = Input(shape=(x_train_title_pad.shape[1],))
    text_input = Input(shape=(x_train_text_pad.shape[1],))

    # Get the predictions from the BiLSTM model
    lstm_predictions = model_BiLSTM([title_input, text_input])

    # Get the predictions from the CNN model
    cnn_predictions = model_CNN([title_input, text_input])

    # Concatenate the predictions
    concatenated_predictions = Concatenate()([lstm_predictions, cnn_predictions])

    # Add a dense layer
    dense_layer = Dense(64, activation='relu')(concatenated_predictions)

    # Add another dense layer for the final output
    output_layer = Dense(5, activation='softmax')(dense_layer)

    ensemble_model = Model(inputs=[title_input, text_input], outputs=output_layer)

    return ensemble_model

# Build the ensemble model
model_ensemble_bilstm_cnn = build_ensemble_model(model_BiLSTM, model_CNN)
model_ensemble_bilstm_cnn.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
model_ensemble_bilstm_cnn.summary()

# from keras.utils import plot_model

# plot_model(model_ensemble_bilstm_cnn, to_file='modelensembleCNNBilstm.png', show_shapes=True, show_layer_names=True)

history = model_ensemble_bilstm_cnn.fit(
    [np.array(x_train_title_pad), np.array(x_train_text_pad)],
    y_train,
    epochs=EPOCH,
    batch_size=4,
    verbose=1,
    validation_data=([np.array(x_test_title_pad), np.array(x_test_text_pad)], y_test)
)

# """# **TRANSFORMER MODEL**"""

# glove_model = Glove.load(path + 'gloveModel.model')

# class PositionalEncoding(nn.Module):
#     def __init__(self, d_model, max_seq_length):
#         super(PositionalEncoding, self).__init__()

#         pe = torch.zeros(max_seq_length, d_model)
#         position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)
#         div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))

#         pe[:, 0::2] = torch.sin(position * div_term)
#         pe[:, 1::2] = torch.cos(position * div_term)

#         self.register_buffer('pe', pe.unsqueeze(0))

#     def forward(self, x):
#         return x + self.pe[:, :x.size(1)]

# class MultiHeadAttention(nn.Module):
#     def __init__(self, d_model, num_heads):
#         super(MultiHeadAttention, self).__init__()
#         # Ensure that the model dimension (d_model) is divisible by the number of heads
#         assert d_model % num_heads == 0, "d_model must be divisible by num_heads"

#         # Initialize dimensions
#         self.d_model = d_model # Model's dimension
#         self.num_heads = num_heads # Number of attention heads
#         self.d_k = d_model // num_heads # Dimension of each head's key, query, and value

#         # Linear layers for transforming inputs
#         self.W_q = nn.Linear(d_model, d_model) # Query transformation
#         self.W_k = nn.Linear(d_model, d_model) # Key transformation
#         self.W_v = nn.Linear(d_model, d_model) # Value transformation
#         self.W_o = nn.Linear(d_model, d_model) # Output transformation

#     def scaled_dot_product_attention(self, Q, K, V, mask=None):
#         # Calculate attention scores
#         attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)

#         # Apply mask if provided (useful for preventing attention to certain parts like padding)
#         if mask is not None:
#             attn_scores = attn_scores.masked_fill(mask == 0, -1e9)

#         # Softmax is applied to obtain attention probabilities
#         attn_probs = torch.softmax(attn_scores, dim=-1)

#         # Multiply by values to obtain the final output
#         output = torch.matmul(attn_probs, V)
#         return output

#     def split_heads(self, x):
#         # Reshape the input to have num_heads for multi-head attention
#         batch_size, seq_length, d_model = x.size()
#         return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)

#     def combine_heads(self, x):
#         # Combine the multiple heads back to original shape
#         batch_size, _, seq_length, d_k = x.size()
#         return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)

#     def forward(self, Q, K, V, mask=None):
#         # Apply linear transformations and split heads
#         Q = self.split_heads(self.W_q(Q))
#         K = self.split_heads(self.W_k(K))
#         V = self.split_heads(self.W_v(V))

#         # Perform scaled dot-product attention
#         attn_output = self.scaled_dot_product_attention(Q, K, V, mask)

#         # Combine heads and apply output transformation
#         output = self.W_o(self.combine_heads(attn_output))
#         return output

# class PositionWiseFeedForward(nn.Module):
#     def __init__(self, d_model, d_ff):
#         super(PositionWiseFeedForward, self).__init__()
#         self.fc1 = nn.Linear(d_model, d_ff)
#         self.fc2 = nn.Linear(d_ff, d_model)
#         self.relu = nn.ReLU()

#     def forward(self, x):
#         return self.fc2(self.relu(self.fc1(x)))

# class EncoderLayer(nn.Module):
#     def __init__(self, d_model, num_heads, d_ff, dropout):
#         super(EncoderLayer, self).__init__()
#         self.self_attn = MultiHeadAttention(d_model, num_heads)
#         self.feed_forward = PositionWiseFeedForward(d_model, d_ff)
#         self.norm1 = nn.LayerNorm(d_model)
#         self.norm2 = nn.LayerNorm(d_model)
#         self.dropout = nn.Dropout(dropout)

#     def forward(self, x, mask):
#         attn_output = self.self_attn(x, x, x, mask)
#         x = self.norm1(x + self.dropout(attn_output))
#         ff_output = self.feed_forward(x)
#         x = self.norm2(x + self.dropout(ff_output))
#         return x

# class DecoderLayer(nn.Module):
#     def __init__(self, d_model, num_heads, d_ff, dropout):
#         super(DecoderLayer, self).__init__()
#         self.self_attn = MultiHeadAttention(d_model, num_heads)
#         self.cross_attn = MultiHeadAttention(d_model, num_heads)
#         self.feed_forward = PositionWiseFeedForward(d_model, d_ff)
#         self.norm1 = nn.LayerNorm(d_model)
#         self.norm2 = nn.LayerNorm(d_model)
#         self.norm3 = nn.LayerNorm(d_model)
#         self.dropout = nn.Dropout(dropout)

#     def forward(self, x, enc_output, src_mask, tgt_mask):
#         attn_output = self.self_attn(x, x, x, tgt_mask)
#         x = self.norm1(x + self.dropout(attn_output))
#         attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)
#         x = self.norm2(x + self.dropout(attn_output))
#         ff_output = self.feed_forward(x)
#         x = self.norm3(x + self.dropout(ff_output))
#         return x

# class Transformer(nn.Module):
#     def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout):
#         super(Transformer, self).__init__()
#         self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)
#         self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)
#         self.positional_encoding = PositionalEncoding(d_model, max_seq_length)

#         self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])
#         self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])

#         self.fc = nn.Linear(d_model, tgt_vocab_size)
#         self.dropout = nn.Dropout(dropout)

#     def generate_mask(self, src, tgt):
#         src_mask = (src != 0).unsqueeze(1).unsqueeze(2)
#         tgt_mask = (tgt != 0).unsqueeze(1)
#         seq_length = tgt.size(1)
#         nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool()
#         tgt_mask = tgt_mask & nopeak_mask
#         return src_mask, tgt_mask

#     def forward(self, src, tgt):
#         src_mask, tgt_mask = self.generate_mask(src, tgt)
#         src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src)))
#         tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(tgt)))

#         enc_output = src_embedded
#         for enc_layer in self.encoder_layers:
#             enc_output = enc_layer(enc_output, src_mask)

#         dec_output = tgt_embedded
#         for dec_layer in self.decoder_layers:
#             dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)

#         output = self.fc(dec_output)
#         return output

# """TITLE"""

# # Preparation
# src_vocab_size = len(glove_model.dictionary)
# tgt_vocab_size = 5
# d_model = 512
# num_heads = 64
# num_layers = 6
# d_ff = 2048
# max_seq_length = 100
# dropout = 0.1
# batch_size = 64  # Adjust batch size as needed

# train_title_x = [torch.from_numpy(element) for element in train_title]
# train_rating_y = [torch.from_numpy(element) for element in train_rating]
# dataset = TensorDataset(torch.stack(train_title_x), torch.stack(train_rating_y))

# # Tạo DataLoader từ TensorDataset
# data_loader = DataLoader(dataset, batch_size=batch_size)  # shuffle=True để trộn dữ liệu

# transformer = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout)

# len(train_rating)

# criterion = nn.CrossEntropyLoss(ignore_index=0)
# optimizer = optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)

# transformer.train()

# for epoch in range(100):
#     for src_data, tgt_data in data_loader:
#       src_data = src_data.long()
#       tgt_data = tgt_data.long()
#       optimizer.zero_grad()
#       output = transformer(src_data, tgt_data)
#       loss = criterion(output.contiguous().view(-1, tgt_vocab_size), tgt_data[:, 1:].contiguous().view(-1))
#       loss.backward()
#       optimizer.step()
#       print(f"Epoch: {epoch+1}, Loss: {loss.item()}")