# -*- coding: utf-8 -*-
# """Hotel Customer's Sentiment Analyse

# Automatically generated by Colaboratory.

# Original file is located at
#     https://colab.research.google.com/drive/1Dr7DdrEFWy8EP4sNW_hOLLm5Kg9Z8VKa

# ## **GETTING STARTED**
# """

# !pip install glove-python3
# !pip install underthesea
path = '/content/drive/MyDrive/WE-GloVe/'

# from google.colab import drive
# drive.mount('/content/drive')

from glove import Corpus, Glove
import pandas as pd
import numpy as np
import re
from underthesea import word_tokenize
import math
from tensorflow.keras import utils
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Flatten, Dense
from tensorflow.keras import utils
from keras.layers import Input, Embedding, LSTM, Dropout, Dense, concatenate
from keras.models import Model

# device_name = tf.test.gpu_device_name()
# if device_name != '/device:GPU:0':
#   raise SystemError('GPU device not found')
# print('Found GPU at: {}'.format(device_name))

# """## **GET TRAIN DATA**"""

# Dataset Prepare
def getData(file_name):
  file = pd.read_csv(path + file_name)

  title = pd.Series([re.sub(r'\s+', ' ', sent) for sent in file['title'].apply(str)])
  text = pd.Series([re.sub(r'\s+', ' ', sent) for sent in file['text'].apply(str)])

  return title, text, utils.to_categorical(file['rating'])

x_train_title, x_train_text, y_train = getData('train.csv')
x_test_title, x_test_text, y_test = getData('test.csv')

# Cho thấy bộ dữ liệu phân bố không đồng đều
# seaborn.countplot(x = 'rating', data = train_file)

# """## **DATA PREPARING STAGE**

# Tokenize data
# """

def tokenize_data(title, text):
  arr_title = [word_tokenize(sentence, format='text') for sentence in title]
  arr_text = [word_tokenize(sentence, format='text') for sentence in text]

  return arr_title, arr_text

x_train_title, x_train_text = tokenize_data(x_train_title, x_train_text)

# """Convert to sequences"""

tokenizer = Tokenizer()

tokenizer.fit_on_texts([x_train_title, x_train_text])

x_train_title_sequence = tokenizer.texts_to_sequences(x_train_title)
x_train_text_sequence = tokenizer.texts_to_sequences(x_train_text)
x_test_title_sequence = tokenizer.texts_to_sequences(x_test_title)
x_test_text_sequence = tokenizer.texts_to_sequences(x_test_text)

# """Padding sequences to the same dimensions"""

vocab_size = len(tokenizer.word_index) + 1
MAX_LEN = 512

x_train_title_pad = pad_sequences(x_train_title_sequence, padding = 'post', maxlen = MAX_LEN)
x_train_text_pad = pad_sequences(x_train_text_sequence, padding = 'post', maxlen = MAX_LEN)
x_test_title_pad = pad_sequences(x_test_title_sequence, padding = 'post', maxlen = MAX_LEN)
x_test_text_pad = pad_sequences(x_test_text_sequence, padding = 'post', maxlen = MAX_LEN)

# """## **GLOVE EMBEDDING IMPLEMENTATION AND USAGE**"""

glove = Glove.load(path + 'gloveModel.model')
emb_dict = dict()

for word in list(glove.dictionary.keys()):
  emb_dict[word] = glove.word_vectors[glove.dictionary[word]]

emb_matrix = np.zeros((vocab_size, MAX_LEN))
for word, index in tokenizer.word_index.items():
  emb_vector = emb_dict.get(word)
  if emb_vector is not None:
    emb_matrix[index] = emb_vector

EPOCH = 100
BATCH_SIZE = 4

# """## **LSTM**

# Define layer
# """

embedding_dim = 128
hidden_size = 256

# Đầu vào cho title
title_input = Input(shape=(x_train_title_pad.shape[1],))
title_embedding = Embedding(vocab_size, embedding_dim, input_length=x_train_title_pad.shape[1])(title_input)
title_lstm = LSTM(hidden_size, return_sequences=True)(title_embedding)
title_lstm_dropout = Dropout(0.2)(title_lstm)
title_lstm_final = LSTM(hidden_size)(title_lstm_dropout)

# Đầu vào cho text
text_input = Input(shape=(x_train_text_pad.shape[1],))
text_embedding = Embedding(vocab_size, embedding_dim, input_length=x_train_text_pad.shape[1])(text_input)
text_lstm = LSTM(hidden_size, return_sequences=True)(text_embedding)
text_lstm_dropout = Dropout(0.2)(text_lstm)
text_lstm_final = LSTM(hidden_size)(text_lstm_dropout)

# Kết hợp hai đầu vào
combined = concatenate([title_lstm_final, text_lstm_final])

# Các bước còn lại của mô hình
dense1 = Dense(128, activation='relu')(combined)
output = Dense(y_train.shape[1], activation='softmax')(dense1)

# Xây dựng mô hình
model_LSTM = Model(inputs=[title_input, text_input], outputs=output)

model_LSTM.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
model_LSTM.summary()

from keras.utils import plot_model

plot_model(model_LSTM, to_file='model.png', show_shapes=True, show_layer_names=True)

model_LSTM_history = model_LSTM.fit(
    [np.array(x_train_title_pad), np.array(x_train_text_pad)],
    y_train,
    epochs=EPOCH,
    batch_size=4,
    verbose=1,
    validation_data=([np.array(x_test_title_pad), np.array(x_test_text_pad)], y_test)
)

# """## **TRANSFORMER MODEL**"""

# glove_model = Glove.load(path + 'gloveModel.model')

# class PositionalEncoding(nn.Module):
#     def __init__(self, d_model, max_seq_length):
#         super(PositionalEncoding, self).__init__()

#         pe = torch.zeros(max_seq_length, d_model)
#         position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)
#         div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))

#         pe[:, 0::2] = torch.sin(position * div_term)
#         pe[:, 1::2] = torch.cos(position * div_term)

#         self.register_buffer('pe', pe.unsqueeze(0))

#     def forward(self, x):
#         return x + self.pe[:, :x.size(1)]

# class MultiHeadAttention(nn.Module):
#     def __init__(self, d_model, num_heads):
#         super(MultiHeadAttention, self).__init__()
#         # Ensure that the model dimension (d_model) is divisible by the number of heads
#         assert d_model % num_heads == 0, "d_model must be divisible by num_heads"

#         # Initialize dimensions
#         self.d_model = d_model # Model's dimension
#         self.num_heads = num_heads # Number of attention heads
#         self.d_k = d_model // num_heads # Dimension of each head's key, query, and value

#         # Linear layers for transforming inputs
#         self.W_q = nn.Linear(d_model, d_model) # Query transformation
#         self.W_k = nn.Linear(d_model, d_model) # Key transformation
#         self.W_v = nn.Linear(d_model, d_model) # Value transformation
#         self.W_o = nn.Linear(d_model, d_model) # Output transformation

#     def scaled_dot_product_attention(self, Q, K, V, mask=None):
#         # Calculate attention scores
#         attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)

#         # Apply mask if provided (useful for preventing attention to certain parts like padding)
#         if mask is not None:
#             attn_scores = attn_scores.masked_fill(mask == 0, -1e9)

#         # Softmax is applied to obtain attention probabilities
#         attn_probs = torch.softmax(attn_scores, dim=-1)

#         # Multiply by values to obtain the final output
#         output = torch.matmul(attn_probs, V)
#         return output

#     def split_heads(self, x):
#         # Reshape the input to have num_heads for multi-head attention
#         batch_size, seq_length, d_model = x.size()
#         return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)

#     def combine_heads(self, x):
#         # Combine the multiple heads back to original shape
#         batch_size, _, seq_length, d_k = x.size()
#         return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)

#     def forward(self, Q, K, V, mask=None):
#         # Apply linear transformations and split heads
#         Q = self.split_heads(self.W_q(Q))
#         K = self.split_heads(self.W_k(K))
#         V = self.split_heads(self.W_v(V))

#         # Perform scaled dot-product attention
#         attn_output = self.scaled_dot_product_attention(Q, K, V, mask)

#         # Combine heads and apply output transformation
#         output = self.W_o(self.combine_heads(attn_output))
#         return output

# class PositionWiseFeedForward(nn.Module):
#     def __init__(self, d_model, d_ff):
#         super(PositionWiseFeedForward, self).__init__()
#         self.fc1 = nn.Linear(d_model, d_ff)
#         self.fc2 = nn.Linear(d_ff, d_model)
#         self.relu = nn.ReLU()

#     def forward(self, x):
#         return self.fc2(self.relu(self.fc1(x)))

# class EncoderLayer(nn.Module):
#     def __init__(self, d_model, num_heads, d_ff, dropout):
#         super(EncoderLayer, self).__init__()
#         self.self_attn = MultiHeadAttention(d_model, num_heads)
#         self.feed_forward = PositionWiseFeedForward(d_model, d_ff)
#         self.norm1 = nn.LayerNorm(d_model)
#         self.norm2 = nn.LayerNorm(d_model)
#         self.dropout = nn.Dropout(dropout)

#     def forward(self, x, mask):
#         attn_output = self.self_attn(x, x, x, mask)
#         x = self.norm1(x + self.dropout(attn_output))
#         ff_output = self.feed_forward(x)
#         x = self.norm2(x + self.dropout(ff_output))
#         return x

# class DecoderLayer(nn.Module):
#     def __init__(self, d_model, num_heads, d_ff, dropout):
#         super(DecoderLayer, self).__init__()
#         self.self_attn = MultiHeadAttention(d_model, num_heads)
#         self.cross_attn = MultiHeadAttention(d_model, num_heads)
#         self.feed_forward = PositionWiseFeedForward(d_model, d_ff)
#         self.norm1 = nn.LayerNorm(d_model)
#         self.norm2 = nn.LayerNorm(d_model)
#         self.norm3 = nn.LayerNorm(d_model)
#         self.dropout = nn.Dropout(dropout)

#     def forward(self, x, enc_output, src_mask, tgt_mask):
#         attn_output = self.self_attn(x, x, x, tgt_mask)
#         x = self.norm1(x + self.dropout(attn_output))
#         attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)
#         x = self.norm2(x + self.dropout(attn_output))
#         ff_output = self.feed_forward(x)
#         x = self.norm3(x + self.dropout(ff_output))
#         return x

# class Transformer(nn.Module):
#     def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout):
#         super(Transformer, self).__init__()
#         self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)
#         self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)
#         self.positional_encoding = PositionalEncoding(d_model, max_seq_length)

#         self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])
#         self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])

#         self.fc = nn.Linear(d_model, tgt_vocab_size)
#         self.dropout = nn.Dropout(dropout)

#     def generate_mask(self, src, tgt):
#         src_mask = (src != 0).unsqueeze(1).unsqueeze(2)
#         tgt_mask = (tgt != 0).unsqueeze(1)
#         seq_length = tgt.size(1)
#         nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool()
#         tgt_mask = tgt_mask & nopeak_mask
#         return src_mask, tgt_mask

#     def forward(self, src, tgt):
#         src_mask, tgt_mask = self.generate_mask(src, tgt)
#         src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src)))
#         tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(tgt)))

#         enc_output = src_embedded
#         for enc_layer in self.encoder_layers:
#             enc_output = enc_layer(enc_output, src_mask)

#         dec_output = tgt_embedded
#         for dec_layer in self.decoder_layers:
#             dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)

#         output = self.fc(dec_output)
#         return output

# # """TITLE"""

# # Preparation
# src_vocab_size = len(glove_model.dictionary)
# tgt_vocab_size = 5
# d_model = 512
# num_heads = 64
# num_layers = 6
# d_ff = 2048
# max_seq_length = 100
# dropout = 0.1
# batch_size = 64  # Adjust batch size as needed

# train_title_x = [torch.from_numpy(element) for element in train_title]
# train_rating_y = [torch.from_numpy(element) for element in train_rating]
# dataset = TensorDataset(torch.stack(train_title_x), torch.stack(train_rating_y))

# # Tạo DataLoader từ TensorDataset
# data_loader = DataLoader(dataset, batch_size=batch_size)  # shuffle=True để trộn dữ liệu

# transformer = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout)

# len(train_rating)

# criterion = nn.CrossEntropyLoss(ignore_index=0)
# optimizer = optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)

# transformer.train()

# for epoch in range(100):
#     for src_data, tgt_data in data_loader:
#       src_data = src_data.long()
#       tgt_data = tgt_data.long()
#       optimizer.zero_grad()
#       output = transformer(src_data, tgt_data)
#       loss = criterion(output.contiguous().view(-1, tgt_vocab_size), tgt_data[:, 1:].contiguous().view(-1))
#       loss.backward()
#       optimizer.step()
#       print(f"Epoch: {epoch+1}, Loss: {loss.item()}")